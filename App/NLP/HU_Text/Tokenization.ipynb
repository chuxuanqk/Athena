{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice = gutenberg.raw(fileids='carroll-alice.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "144395"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(nltk.__file__)\n",
    "print(type(alice))\n",
    "alice.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\"\n",
      " '\\n'\n",
      " 'CHAPTER I. Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was')\n"
     ]
    }
   ],
   "source": [
    "pprint(alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>句子切分(sentence tokenization)</H2>\n",
    "* sent_tokenize\n",
    "* PunktSentenceTokenizer\n",
    "* RegexpTokenizer\n",
    "* 预先训练的句子切分模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function sent_tokenize in module nltk.tokenize:\n",
      "\n",
      "sent_tokenize(text, language='english')\n",
      "    Return a sentence-tokenized copy of *text*,\n",
      "    using NLTK's recommended sentence tokenizer\n",
      "    (currently :class:`.PunktSentenceTokenizer`\n",
      "    for the specified language).\n",
      "    \n",
      "    :param text: text to split into sentences\n",
      "    :param language: the model name in the Punkt corpus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nltk.sent_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize\n",
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(alice_sentences))\n",
    "len(alice_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 德语切分\n",
    "from nltk.corpus import europarl_raw\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157171"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(german_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' \\n'\n",
      " 'Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. '\n",
      " 'Dezember unterbrochene Sit')\n"
     ]
    }
   ],
   "source": [
    "pprint(german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认的sent_tokenize切分器\n",
    "german_sentences_def = default_st(text=german_text, language='german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "# 加载一个german text预先切分模型到PunktSentenceTokenizer中(预先训练的句子切分模型)\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "print(type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# 比较默认切分器和预训练切分器的区别\n",
    "print(german_sentences_def == german_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "for sent in german_sentences[0:5]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\"\n",
      " '\\n'\n",
      " 'CHAPTER I. Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!',\n",
      " \"I shall be late!'\"]\n"
     ]
    }
   ],
   "source": [
    "# 使用默认的PunktSentenceTokenizer类实现句子切分\n",
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "alice_senteces = punkt_st.tokenize(alice)\n",
    "pprint(alice_senteces[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=4>sent_tokenize和PunktSentenceTokenizer切分的句子有一点区别</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\"\n",
      " '\\n'\n",
      " 'CHAPTER I. Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\\n\"\n",
      " '\\n'\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " '\\n'\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!',\n",
      " \"I shall be late!' (when she thought it over afterwards, it\\n\"\n",
      " 'occurred to her that she ought to have wondered at this, but at the time\\n'\n",
      " 'it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\\n'\n",
      " 'OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\\n'\n",
      " 'Alice started to her feet, for it flashed across her mind that she had\\n'\n",
      " 'never before seen a rabbit with either a waistcoat-pocket, or a watch\\n'\n",
      " 'to take out of it, and burning with curiosity, she ran across the field\\n'\n",
      " 'after it, and fortunately was just in time to see it pop down a large\\n'\n",
      " 'rabbit-hole under the hedge.',\n",
      " '\\n'\n",
      " 'In another moment down went Alice after it, never once considering how\\n'\n",
      " 'in the world she was to get out again.']\n"
     ]
    }
   ],
   "source": [
    "# 使用RegexpTOkenizer类的实例将文本切分为句子，基于正则表达式的模式\n",
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.])(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN, gaps=True)\n",
    "re_alice_snetences = regex_st.tokenize(alice)\n",
    "pprint(re_alice_snetences[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2>词语切分(word tokenization)</H2>\n",
    "* word_tokenize\n",
    "* TreebankWordTokenizer\n",
    "* RegexpTokenizer\n",
    "* 从RegexpTokenizer继承的切分器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN, gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN, gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# 获取开始和结束的每个token的下标，并打印出来\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print(word_indices)\n",
    "print([sentence[start:end] for start, end in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RegexpTokenizer类的派生类\n",
    "* WordPunktTokenizer使用  r'\\w+|[^\\w\\s]+' 模式将句子切分成独立的字母和非字母标识\n",
    "* WhitespaceTokenizer基于诸如缩进符、换行符及空格的空白字符将句子分割成单词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
